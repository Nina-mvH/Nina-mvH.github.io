---
layout: post
title: Week 10
---

I spent the bginning on this week continuing to train and test neural netwoks...Plotted for 0.5 and 0.2, respectivley

### Modification 23
- I halved the original channel numbers and used an early stopping patience of 25
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- Before training, I loaded the weights of the pretrained model from modification 1.27 for a starting point
- The network stopped training at epoch 250/1000 with a training loss of 0.0226 and a validation loss of 0.0303
- When tested on the unseen cube of data, this network had a loss of 0.573
![image](https://github.com/user-attachments/assets/3932b015-ab6d-4f31-88e6-11ef6b42bc66)
![image](https://github.com/user-attachments/assets/d1339c9c-66ce-450c-9ef7-d1b0d5a6cffe)

At this point, I decided to look into how to better prevent overfitting. Though my implementation of early stopping had helped, I still noticed a significant difference between validation loss and training loss; an example of this difference, from modification 22, can be seen here.
![image](https://github.com/user-attachments/assets/4f7d3474-3454-4f9f-aec5-e17adbef540d)
In order to remedy this issue, I began implementing dropout...

### Modification 24
- Keeping the original channel numbers, I added bath normalization and droupout, and used an early stopping patience of 25
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- The network stopped training at epoch 130/1000 with a training loss of 0.0261 and a validation loss of 0.0305 (this difference was much improved compared to before)
- When tested on the unseen cube of data, this network had a loss of 0.267
![image](https://github.com/user-attachments/assets/e9ba519c-a896-407e-b8f3-3d6a79a9e569)
![image](https://github.com/user-attachments/assets/d9476ff9-0fd3-425a-afdc-fd4149ae469e)
![image](https://github.com/user-attachments/assets/18aa1cdf-4e73-4cc9-b2b4-8635f07906b4)

I also determined that my model was learning too fast. The majority of the plots generated from training show a sharp decrease in loss and a fast platoe, which appears to be due to a [learning rate that is too high](https://towardsdatascience.com/https-medium-com-dashingaditya-rakhecha-understanding-learning-rate-dd5da26bb6de). 

