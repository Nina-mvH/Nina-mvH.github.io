---
layout: post
title: Week 10
---

I spent the bginning on this week continuing to train and test neural netwoks, which can be seen below; binary map plots were made for thresholds of 0.5 and 0.2 respectively, and the differences in the nerual networks compared to their immediate predecessor have been made bold. I spent the latter part of the week writing my final report for the summer which can be found on the homepage of this website. It donesn't include any of the interesting analysis work that I've done that could be used by the lab in future publications, but provides an overview of the development of the neural networks. 

### Modification 23
- I halved the original channel numbers and used an early stopping patience of 25
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- Before training, I loaded the weights of the pretrained model from modification 1.27 for a starting point
- The network stopped training at epoch 250/1000 with a training loss of 0.0226 and a validation loss of 0.0303
- When tested on the unseen cube of data, this network had a loss of 0.573
![image](https://github.com/user-attachments/assets/3932b015-ab6d-4f31-88e6-11ef6b42bc66)
![image](https://github.com/user-attachments/assets/d1339c9c-66ce-450c-9ef7-d1b0d5a6cffe)

At this point, I decided to look into how to better prevent overfitting. Though my implementation of early stopping had helped, I still noticed a significant difference between validation loss and training loss; an example of this difference, from modification 22, can be seen here.
![image](https://github.com/user-attachments/assets/4f7d3474-3454-4f9f-aec5-e17adbef540d)
In order to remedy this issue, I began implementing dropout...

### Modification 24
- Keeping the original channel numbers, I added bath normalization *and droupout*, and used an early stopping patience of 25
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- The network stopped training at epoch 130/1000 with a training loss of 0.0261 and a validation loss of 0.0305 (this difference was much improved compared to before)
- When tested on the unseen cube of data, this network had a loss of 0.267
![image](https://github.com/user-attachments/assets/e9ba519c-a896-407e-b8f3-3d6a79a9e569)
![image](https://github.com/user-attachments/assets/d9476ff9-0fd3-425a-afdc-fd4149ae469e)
![image](https://github.com/user-attachments/assets/18aa1cdf-4e73-4cc9-b2b4-8635f07906b4)

I also determined that my model was learning too fast. The majority of the plots generated from training show a sharp decrease in loss and a fast platoe, which appears to be due to a [learning rate that is too high](https://towardsdatascience.com/https-medium-com-dashingaditya-rakhecha-understanding-learning-rate-dd5da26bb6de). Therefore, I then focused on decreasing the learning rate, which had previously stayed at the defualt 0.001.

### Modification 25
- Keeping the original channel numbers, I added bath normalization and droupout, and used an early stopping patience of 25
- A learning rate of *0.0001* was fed to the Adam optimizer
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- The network stopped training at epoch 130/1000 with a training loss of 0.0312 and a validation loss of 0.0356, and still appeared to learn too quickly ![image](https://github.com/user-attachments/assets/2615532b-a88e-4543-a4ec-426c7d21664b)
- When tested on the unseen cube of data, this network had a loss of 0.299
![image](https://github.com/user-attachments/assets/799b0a7e-f921-4574-9651-309a1c1f385e)
![image](https://github.com/user-attachments/assets/7796e2ae-709e-4332-88dd-1b7f1482a497)

### Modification 26
- Keeping the original channel numbers, I added bath normalization and droupout, and used an early stopping patience of 25
- A learning rate of *0.00005* was fed to the Adam optimizer
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- The network stopped training at epoch 220/1000 with a training loss of 0.0295 and a validation loss of 0.0425, and still appeared to learn too quickly ![image](https://github.com/user-attachments/assets/b5d73019-2546-4c36-8d1d-ade88361594e)
- When tested on the unseen cube of data, this network had a loss of 0.284
![image](https://github.com/user-attachments/assets/ce107b71-d8f3-4c13-a850-515504adc876)
![image](https://github.com/user-attachments/assets/69e23611-d06b-4d1b-9ff7-182730d5aa8f)

### Modification 27
- Keeping the original channel numbers, I added bath normalization and droupout, and used an early stopping patience of 25
- A learning rate of *0.00001* was fed to the Adam optimizer
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- The network stopped training at epoch 260/1000 with a training loss of 0.0382 and a validation loss of 0.0372, and appeared to have an improved learning rate ![image](https://github.com/user-attachments/assets/ee0da869-f133-4b35-b533-8e4d081e2ef4)
- When tested on the unseen cube of data, this network had a loss of 0.274
![image](https://github.com/user-attachments/assets/c4fdbb1d-8c0e-450f-a893-23dcc1053c99)
![image](https://github.com/user-attachments/assets/394b0db1-a8fe-4277-8369-b24df8077d53)

### Modification 28
- Keeping the original channel numbers, I added bath normalization and droupout, and used an early stopping patience of 25
- A learning rate of *0.000005* was fed to the Adam optimizer
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- The network stopped training at epoch 230/1000 with a training loss of 0.0491 and a validation loss of 0.0490, and appeared to have an improved learning rate, though learning took significantly longer ![image](https://github.com/user-attachments/assets/9085d1a3-a9c7-4ac2-8223-4524ac38b646)
- When tested on the unseen cube of data, this network had a loss of 0.247
![image](https://github.com/user-attachments/assets/2f90a556-8716-47d6-8835-833e01a98a41)
![image](https://github.com/user-attachments/assets/f363c98d-84ed-457b-b4d1-3f2af36769ef)

After I was happy with the learning rate, I played around with various other things, but these didn't seem to improve the network much...

### Modification 29
- Keeping the original channel numbers, I added bath normalization and droupout, and used an early stopping patience of 25
- A learning rate of 0.000005 was fed to the Adam optimizer, *along with weight_decay=1e-5*
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- The network stopped training at epoch 300/1000 with a training loss of 0.0448 and a validation loss of 0.0461
- When tested on the unseen cube of data, this network had a loss of 0.293
![image](https://github.com/user-attachments/assets/a6cd64dc-b2b5-43fa-ae3c-27e0879f33ab)

### Modification 30
- Keeping the original channel numbers, I added bath normalization and droupout, and used an early stopping patience of 25
- A learning rate of 0.000005 was fed to the Adam optimizer
- The small swarm was divided into *10,092 cubes with 25 voxels overlap* to use for training data
- The network stopped training at epoch 230/1000 with a training loss of 0.0283 and a validation loss of 0.0254, and it once again appeared that the learning rate was too small, but training was taking too long to increase it significantly
- When tested on the unseen cube of data, this network had a loss of 0.288
![image](https://github.com/user-attachments/assets/d9412b23-8a48-4824-bec8-55bd646e95bc)

### Modification 31
- Keeping the original channel numbers, I added bath normalization and droupout, and used an early stopping patience of 25
- A learning rate of 0.000005 was fed to the Adam optimizer
- The small swarm was divided into *605 cubes with 13 voxels overlap* to use for training data, then *this data was duplicated and the copied data was rotated either 90, 180, or 270 degrees to add augmentation to the training data*
- The network stopped training at epoch 90/1000 with a training loss of 0.1654 and a validation loss of 0.0930
- When tested on the unseen cube of data, this network had a loss of 0.338
![image](https://github.com/user-attachments/assets/9fa78da7-3883-4069-ac55-ff60ba48a462)

### Modification 32
- Keeping the original channel numbers, I added bath normalization and droupout, and used an early stopping patience of 25
- A learning rate of *0.000001* was fed to the Adam optimizer
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data, then this data was duplicated and the copied data was rotated either 90, 180, or 270 degrees to add augmentation to the training data
- The network stopped training at epoch 210/1000 with a training loss of 0.1683 and a validation loss of 0.0938
- When tested on the unseen cube of data, this network had a loss of 0.338
