---
layout: post
title: Week 4
---

I started off this week with a meeting with Danielle to talk about what I was able to do last week and interesting next steps I could take. The rest of the week I spent (1) continuing to make changes to the neural network, (2) looking more in-depth into how well the networks were performing, and (3) labelling bees. 

 
1) The Neural Network:
Below are some more modifications I made to the neaural network with the hopes of improving it. About halfway through the week I also was set up with a GPU, so I was able to try some things that were too time intensive on my computer's CPU. See last week's entry for modifications 0-2. Note that all training occured over 1000 epochs.

Modification 3:
- given the starting neural network with 2 encoder and 2 decoder layers, I tried adding another layer to create a deeper network
- This change increased the loss from 3.182e-06 to 1.940e-05 given the training data
- Given test data (the same cube, cropped in an unseen way), the loss increased from 9.292 to 11.238
- Training took a total wall time of 21min 44s on the CPU
- I was quite surprised that making the network deeper decreased performance (at least when trained on one image)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/9a634df8-7788-461b-9e31-9c85ef9a7cd7)

Modification 4:
- Given the starting neural network, I added batch normalization to each convolution layer
- This change increased the loss from 3.182e-06 to 0.001 given the training data
- Loss decreased from 9.292 to 0.546 when tested on the same cube but with a different crop
- Training took a total wall time of 21min 33s on the CPU
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/769a99ae-3c27-4c4b-a27f-c62e86f9be13)


Modification 4, but trained on 8 different crops of my cube:
- Training on 8 different crops of the cube resulted in a training loss of 3.021e-05 and a test loss of 0.781.
- Training took a total wall time of 1h 21min 2s on the CPU
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/e579a217-f45e-46b9-b9c1-60e517410147)

Modification 5:
- After gaining acess to a GPU I decided to combine previous things that seemed to improve performance while also implimenting early stopping to prevent overfitting
- First, I trained a network with doubled channels on 125 different crops of my cube with an early stopping patience of 4
- 
Doubled Channels, batch, many crops
- j
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/9d18c1e7-ca71-4fac-b37a-6d0f5afa0ec4)

Doubled Channels, many crops, early stopping
- 
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/6ff23006-4614-47f9-a3bf-f86b7223d014)


Doubled Channels, many crops, batch norm, early stopping
- 
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/eaa57ac1-ca19-4e44-a570-40f0ddd7c0bd)



![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/aefc22d0-cbcd-4e18-94b7-6d51fd45713a)
