---
layout: post
title: Week 4
---

I started off this week with a meeting with Danielle to talk about what I was able to do last week and interesting next steps I could take. The rest of the week I spent (1) continuing to make changes to the neural network, (2) looking more in-depth into how well the networks were performing, (3) attempted to use my networks on the small swarm, and (4) labelling bees. 


 
1) The Neural Network:
   
Below are some more modifications I made to the neaural network with the hopes of improving it. About halfway through the week I also was set up with a GPU, so I was able to try some things that were too time intensive on my computer's CPU. See last week's entry for modifications 0-2. Note that all training occured over 1000 epochs.


Modification 3:
- given the starting neural network with 2 encoder and 2 decoder layers, I tried adding another layer to create a deeper network
- This change increased the loss from 3.182e-06 to 1.940e-05 given the training data
- Given test data (the same cube, cropped in an unseen way), the loss increased from 9.292 to 11.238
- Training took a total wall time of 21min 44s on the CPU
- I was quite surprised that making the network deeper decreased performance (at least when trained on one image)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/9a634df8-7788-461b-9e31-9c85ef9a7cd7)


Modification 4:
- Given the starting neural network, I added batch normalization to each convolution layer
- This change increased the loss from 3.182e-06 to 0.001 given the training data
- Loss decreased from 9.292 to 0.546 when tested on the same cube but with a different crop
- Training took a total wall time of 21min 33s on the CPU
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/769a99ae-3c27-4c4b-a27f-c62e86f9be13)


Modification 4, but trained on 8 different crops of my cube:
- Training on 8 different crops of the cube resulted in a training loss of 3.021e-05 and a test loss of 0.781
- Training took a total wall time of 1h 21min 2s on the CPU
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/e579a217-f45e-46b9-b9c1-60e517410147)


Modification 5:
- After gaining acess to a GPU I decided to combine previous things that seemed to improve performance
- First, I trained a network with doubled channels and batch normalization on 125 different crops of my cube
- This resulted in a training loss of 1.058e-11 and a test loss of 1.830
- Training took a total wall time of 46min 6s on the GPU
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/9d18c1e7-ca71-4fac-b37a-6d0f5afa0ec4)


Modification 6:
- I then trained another network with doubled channels on 125 different crops of my cube, but also manually implimented early stopping to try to prevent overfitting
- Out of all 125 pieces of training data, 20% were set aside for validation data, and a patience of 4 was used
- After 40 epcohs, the training stopped at a training loss of 0.007 and a validation loss of  0.011
- Training completed after a wall time of 2min 1s on the GPU
- Given an unseen crop of the cube, test loss was 0.003
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/6ff23006-4614-47f9-a3bf-f86b7223d014)


Modification 7:
- I then trained a network with doubled channels, batch normalization, and early stopping on 125 crops of my cube
- After 60 epochs, training stopped at a training loss of 0.0001 and a validation loss of 0.001
- Training completed after a wall time of 2min 58s on the GPU
- Given an unseen crop of the cube, test loss was 0.351
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/eaa57ac1-ca19-4e44-a570-40f0ddd7c0bd)





2) Network Performance Analysis:

Up until this point, the only way I was evaluating how well my networks were doing was through loss and visually comparing the output to my labels, so I decided to try to evaluate some of the outputs in a more thorough way. I first started with the original architecture as a benchmark. Similarly to when I was first analyzing my input data, I started by looking at the connected components of the output given different thresholds (0.1-0.9):
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/96bc3d6f-de26-4748-a010-7a65ee62b47e)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/be85dfe4-016a-425a-8d42-3104c6370e39)
It was apparent that, despite the threshold level, many of the "bees" identified by this network were connected. I visualized the sizes of these components using histograms:
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/3cdbe4ef-47a8-428c-a242-81d3a7c9f0dc)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/5926791d-d0a9-49f3-b273-a9cae760b57f)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/bed94f1a-5ef2-4e4d-bdf5-3bed0eda375f)
So, using this benchmark network, arround 25-30 individual bees were able to be identified, and many appear to be connected.

I then did the same analysis using the network with halved channels, trained on 27 different crops of the cube (at the time, this was my best performing network).
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/af1c4ba0-7c45-4080-abba-d273f6e66d38)
From the above image, with a very low threshold, it is clear that some seperate bees are connected, as shown by the light green masses. Regardless, a significantly higher number of bees is being identified when compared to the benchmark output.
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/cf4bc94e-c867-4df7-8af9-80dc6e10aee5)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/b6a2d73a-b121-4aaa-a459-250468a5ee58)
With higher thresholds, there are less connected components and what appear to be more individual bees, though bees are at higher risk of getting split into two/three seperate pieces.
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/d6a00ba9-28bb-42a9-8cc1-0c7d8014de59)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/8c377331-c5cd-4a6b-a8a2-cd0c50ae2e13)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/bb7f868b-d0ed-4f04-b720-bf10c169cb79)
Given this network's output, there also seems to be a larger variety of bee sizes, ranging from 1 to 533 pixels.
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/3147c0a7-c335-42d1-9fe7-c181e46d82b7)





3) Using the Network(s) on the Small Swarm:

![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/aefc22d0-cbcd-4e18-94b7-6d51fd45713a)





4) Labelling the Small Swarm:

After the end of this week, I was able to finish labelling about ___ bees.
