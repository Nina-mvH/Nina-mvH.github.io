---
layout: post
title: Week 8
---

I started off this week by training neural networks using my labeled small swarm for training data. This process took some time to start due to the large amounts of data constantly causing my notebook to crash, but I was eventually able to modify everything to allow for this increased data size. I trained various networks throughout the rest of the week, and the specifications of the networks and the results can be seen listed below:

Modification 9:
- Keeping the original channel numbers, I used batch normalization and an early stopping patience of 4
- The small swarm was divided into 147 cubes with 0 overlap to use for training data
- The network stopped training at epcoh 10/1000 with a training loss of 0.0523 and a validation loss of 0.0717
- When tested on an unseen cube of data (my original labeled cube), this network had a loss of 0.246
![image](https://github.com/user-attachments/assets/206793c3-1604-4ef9-9df6-10310c911963)
![image](https://github.com/user-attachments/assets/050ee9ad-c207-4e43-8901-9eb91f852828)

Modification 10:
- 


- through 21


Alongside training the networks, I presented my research progress to the rest of the members of the lab for the first time. This presentation went well, and the lab members were excited to see the possibility of a neural network which could give them access to more information about the innerworkings of bee swarms. They also asked numerous questions and provided multiple suggestions which gave me more ideas about future things I could look at. After this presentation, I began to look into some of these things, but due to confidentiality reasons, can't discuss them here.
