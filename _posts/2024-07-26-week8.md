---
layout: post
title: Week 8
---

I started off this week by training neural networks using my labeled small swarm for training data. This process took some time to start due to the large amounts of data constantly causing my notebook to crash, but I was eventually able to modify everything to allow for this increased data size. I trained various networks throughout the rest of the week, and the specifications of the networks and the results can be seen listed below:

### Modification 9:
- Keeping the original channel numbers, I used batch normalization and an early stopping patience of 4
- The small swarm was divided into 147 cubes with 0 overlap to use for training data
- The network stopped training at epcoh 10/1000 with a training loss of 0.0523 and a validation loss of 0.0717
- When tested on an unseen cube of data (my original labeled cube), this network had a loss of 0.246
![image](https://github.com/user-attachments/assets/206793c3-1604-4ef9-9df6-10310c911963)
![image](https://github.com/user-attachments/assets/050ee9ad-c207-4e43-8901-9eb91f852828)

### Modification 10:
- Keeping the original channel numbers, I used batch normalization and an early stopping patience of 50
- The small swarm was divided into 147 cubes with 0 overlap to use for training data
- The network stopped training at epcoh 80/1000 with a training loss of 0.0203 and a validation loss of 0.0551
- When tested on the unseen cube of data, this network had a loss of 0.275
![image](https://github.com/user-attachments/assets/4e69b922-ac3c-4ad8-bf48-af3d08226aba)
![image](https://github.com/user-attachments/assets/d15b0396-bc7b-4a87-b86c-7418b39a56b7)

### Modification 11:
- Keeping the original channel numbers, I used batch normalization and an early stopping patience of 25
- The small swarm was divided into 400 cubes with 10 voxels overlap to use for training data 
- The network stopped training at epcoh 80/1000 with a training loss of 0.0133 and a validation loss of 0.0337
- When tested on the unseen cube of data, this network had a loss of 0.584
![image](https://github.com/user-attachments/assets/cbf5083d-1738-4ff2-be63-921f535a5500)
![image](https://github.com/user-attachments/assets/c46f4a5d-81d3-4d85-b901-cb10a2716280)

### Modification 12:
- I doubled the original channel numbers and used an early stopping patience of 25
- The small swarm was divided into 147 cubes with 0 overlap to use for training data
- The network stopped training at epcoh 80/1000 with a training loss of 0.0197 and a validation loss of 0.0600
- When tested on the unseen cube of data, this network had a loss of 0.392
![image](https://github.com/user-attachments/assets/44e1ba27-7521-41a5-a22a-e692926e6048)
![image](https://github.com/user-attachments/assets/0b88f57e-7679-4a68-a994-846ba9624c2a)

### Modification 13:
- I doubled the original channel numbers and I removed my implementation of early stopping, though I continued to track validation loss at each epoch
- The small swarm was divided into 400 cubes with 10 voxels overlap to use for training data
- After epcoh 1000/1000, the network had a training loss of 0.0000 and a validation loss of 0.2514
- When tested on the unseen cube of data, this network had a loss of 0.695
![image](https://github.com/user-attachments/assets/1fcd2503-6900-44b9-836a-fa5327142fc1)
![image](https://github.com/user-attachments/assets/b8538cb2-b242-487d-8eb0-9c4ebe392abe)

### Modification 14:
- I doubled the original channel numbers, used batch normalization, and an early stopping patience of 25
- The small swarm was divided into 400 cubes with 10 voxels overlap to use for training data
- The network stopped training at epcoh 80/1000 with a training loss of 0.0124 and a validation loss of 0.0294
- When tested on the unseen cube of data, this network had a loss of 0.518
![image](https://github.com/user-attachments/assets/073f1dec-71a3-4695-af5e-d043a594734c)
![image](https://github.com/user-attachments/assets/f046a0d2-90ff-4dea-903a-f1b94e09d5e5)

### Modification 15:
- Keeping the original channel numbers, I used an early stopping patience of 25
- The small swarm was divided into 400 cubes with 10 voxels overlap to use for training data
- The network stopped training at epcoh 70/1000 with a training loss of 0.0109 and a validation loss of 0.0541
- When tested on the unseen cube of data, this network had a loss of 0.451
![image](https://github.com/user-attachments/assets/5d46c3cd-1bba-44b3-95f0-afdb5cc87424)
![image](https://github.com/user-attachments/assets/ce77f0e2-0bef-4f09-a786-b145ccbd9397)

### Modification 16
- Keeping the original channel numbers, I used an early stopping patience of 5
- The small swarm was divided into 400 cubes with 10 voxels overlap to use for training data
- The network stopped training at epcoh 30/1000 with a training loss of 0.0314 and a validation loss of 0.0365
- When tested on the unseen cube of data, this network had a loss of 0.425
![image](https://github.com/user-attachments/assets/e31ef63a-34f6-47bf-8181-4021c34b26f8)
![image](https://github.com/user-attachments/assets/c6e6bc61-e8bc-4a4f-903c-795d0a456c3b)

When looking at the "Residuals:bee-notbee minus labels" figure for the cubes, it may look like these neural networks are performing worse when compared to the ones trained on just the cube. However, the loss of these networks seems to generally be lower, and, unlike the original networks I've trained, these networks have never seen any of the data from this cube. Therefore, I think these networks are perfroming better given unseen data and those original netowrks were slightly overfit to the cube of data they trained on (and were tested on). It's also important to note that the UNet binary output determining if each voxel is a bee or not a bee is based on a threshold (in this case 0.5). I think this threshold might be too high since in the "UNet Output" figure I can see many lighter areas that are not picked up due to the high threshold, so next week I plan to re-test these networks with a lower threshold level to see if that helps with the identification of more bees which are smaller/lighter in color or if this leads to bees being connected.

Alongside training the networks, I presented my research progress to the rest of the members of the lab for the first time. This presentation went well, and the lab members were excited to see the possibility of a neural network which could give them access to more information about the innerworkings of bee swarms. They also asked numerous questions and provided multiple suggestions which gave me more ideas about future things I could look at. After this presentation, I began to look into some of these things, but due to confidentiality reasons, can't discuss them here.
