---
layout: post
title: Week 8
---

I started off this week by training neural networks using my labeled small swarm for training data. This process took some time to start due to the large amounts of data constantly causing my notebook to crash, but I was eventually able to modify everything to allow for this increased data size. I trained various networks throughout the rest of the week, and the specifications of the networks and the results can be seen listed below:

### Modification 9:
- Keeping the original channel numbers, I used batch normalization and an early stopping patience of 4
- The small swarm was divided into 147 cubes with 0 overlap to use for training data
- The network stopped training at epcoh 10/1000 with a training loss of 0.0523 and a validation loss of 0.0717
- When tested on an unseen cube of data (my original labeled cube), this network had a loss of 0.246
![image](https://github.com/user-attachments/assets/206793c3-1604-4ef9-9df6-10310c911963)
![image](https://github.com/user-attachments/assets/050ee9ad-c207-4e43-8901-9eb91f852828)

### Modification 10:
- Keeping the original channel numbers, I used batch normalization and an early stopping patience of 50
- The small swarm was divided into 147 cubes with 0 overlap to use for training data
- The network stopped training at epcoh 80/1000 with a training loss of 0.0203 and a validation loss of 0.0551
- When tested on the unseen cube of data, this network had a loss of 0.275
![image](https://github.com/user-attachments/assets/4e69b922-ac3c-4ad8-bf48-af3d08226aba)
![image](https://github.com/user-attachments/assets/d15b0396-bc7b-4a87-b86c-7418b39a56b7)

### Modification 11:
- Keeping the original channel numbers, I used batch normalization and an early stopping patience of 25
- The small swarm was divided into 400 cubes with 10 voxels overlap to use for training data 
- The network stopped training at epcoh 80/1000 with a training loss of 0.0133 and a validation loss of 0.0337
- When tested on the unseen cube of data, this network had a loss of 0.584
![image](https://github.com/user-attachments/assets/cbf5083d-1738-4ff2-be63-921f535a5500)
![image](https://github.com/user-attachments/assets/c46f4a5d-81d3-4d85-b901-cb10a2716280)

### Modification 12:
- I doubled the original channel numbers and used an early stopping patience of 25
- The small swarm was divided into 147 cubes with 0 overlap to use for training data
- The network stopped training at epcoh 80/1000 with a training loss of 0.0197 and a validation loss of 0.0600
- When tested on the unseen cube of data, this network had a loss of 0.392
![image](https://github.com/user-attachments/assets/44e1ba27-7521-41a5-a22a-e692926e6048)
![image](https://github.com/user-attachments/assets/0b88f57e-7679-4a68-a994-846ba9624c2a)

### Modification 13:
- I doubled the original channel numbers and removed my implementation of early stopping, though I continued to track validation loss at each epoch
- The small swarm was divided into 400 cubes with 10 voxels overlap to use for training data
- After epcoh 1000/1000, the network had a training loss of 0.0000 and a validation loss of 0.2514
- When tested on the unseen cube of data, this network had a loss of 0.695
![image](https://github.com/user-attachments/assets/1fcd2503-6900-44b9-836a-fa5327142fc1)
![image](https://github.com/user-attachments/assets/b8538cb2-b242-487d-8eb0-9c4ebe392abe)

### Modification 14:
- I doubled the original channel numbers, used batch normalization, and an early stopping patience of 25
- The small swarm was divided into 400 cubes with 10 voxels overlap to use for training data
- The network stopped training at epcoh 80/1000 with a training loss of 0.0124 and a validation loss of 0.0294
- When tested on the unseen cube of data, this network had a loss of 0.518
![image](https://github.com/user-attachments/assets/073f1dec-71a3-4695-af5e-d043a594734c)
![image](https://github.com/user-attachments/assets/f046a0d2-90ff-4dea-903a-f1b94e09d5e5)

### Modification 15:
- Keeping the original channel numbers, I used an early stopping patience of 25
- The small swarm was divided into 400 cubes with 10 voxels overlap to use for training data
- The network stopped training at epcoh 70/1000 with a training loss of 0.0109 and a validation loss of 0.0541
- When tested on the unseen cube of data, this network had a loss of 0.451
![image](https://github.com/user-attachments/assets/5d46c3cd-1bba-44b3-95f0-afdb5cc87424)
![image](https://github.com/user-attachments/assets/ce77f0e2-0bef-4f09-a786-b145ccbd9397)

Compared to the best neural networks I trained using just the single cube of labled bees, these neural networks seem to be performing worse when given unseesn data--they have a higher loss value and visually it seems that they're missing the outlines of the bees. While this could mean that these networks are worse, I think this actually just proves that the original networks were overfit. Since, for those networks, the "unseen test data" was really just a different crop of the same cube, I believe that the networks had an advantage and were therefore able to label the bees in this same cube well. Therefore, even though my new networks may appear to be peforming worse, I believe they are still an improvement. To test this idea, I used one of my best neural networks trained on the single cube on the small swarm.


Alongside training the networks, I presented my research progress to the rest of the members of the lab for the first time. This presentation went well, and the lab members were excited to see the possibility of a neural network which could give them access to more information about the innerworkings of bee swarms. They also asked numerous questions and provided multiple suggestions which gave me more ideas about future things I could look at. After this presentation, I began to look into some of these things, but due to confidentiality reasons, can't discuss them here.
