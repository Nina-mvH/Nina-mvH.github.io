layout: post
title: Week 9
---

I continued working on neural networks using the small swarm as training data, as shown below.

### Modification 16
- Keeping the original channel numbers, I used an early stopping patience of 5
- The small swarm was divided into 400 cubes with 10 voxels overlap to use for training data
- The network stopped training at epcoh 30/1000 with a training loss of 0.0314 and a validation loss of 0.0365
- When tested on the unseen cube of data, this network had a loss of 0.425
![image](https://github.com/user-attachments/assets/e31ef63a-34f6-47bf-8181-4021c34b26f8)
![image](https://github.com/user-attachments/assets/c6e6bc61-e8bc-4a4f-903c-795d0a456c3b)

### Modification 17
- Keeping the original channel numbers, I used an early stopping patience of 5
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- The network stopped training at epcoh 40/1000 with a training loss of 0.0170 and a validation loss of 0.0328
- When tested on the unseen cube of data, this network had a loss of 0.621
![image](https://github.com/user-attachments/assets/5039002a-f5e5-4101-a919-cb749b639722)
![image](https://github.com/user-attachments/assets/24a12a82-b8c6-49f8-aa73-d4fb552eb521)

### Modification 18
- Keeping the original channel numbers, I used an early stopping patience of 25
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- The network stopped training at epcoh 50/1000 with a training loss of 0.0123 and a validation loss of 0.0365
- When tested on the unseen cube of data, this network had a loss of 0.556
![image](https://github.com/user-attachments/assets/3e35e8bc-a23c-4d5d-8efc-d125606164d7)
![image](https://github.com/user-attachments/assets/6f78cfc4-58fc-4666-8397-8da36864e2c5)

### Modification 19
- Keeping the original channel numbers, I used an early stopping patience of 25
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- The network stopped training at epcoh 50/1000 with a training loss of 0.0123 and a validation loss of 0.0365
- When tested on the unseen cube of data, this network had a loss of 0.556



