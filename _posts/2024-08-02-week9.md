layout: post
title: Week 9
---

I continued working on neural networks using the small swarm as training data, as shown below.

### Modification 16
- Keeping the original channel numbers, I used an early stopping patience of 5
- The small swarm was divided into 400 cubes with 10 voxels overlap to use for training data
- The network stopped training at epcoh 30/1000 with a training loss of 0.0314 and a validation loss of 0.0365
- When tested on the unseen cube of data, this network had a loss of 0.425
![image](https://github.com/user-attachments/assets/e31ef63a-34f6-47bf-8181-4021c34b26f8)
![image](https://github.com/user-attachments/assets/c6e6bc61-e8bc-4a4f-903c-795d0a456c3b)

### Modification 17
- Keeping the original channel numbers, I used an early stopping patience of 5
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- The network stopped training at epcoh 40/1000 with a training loss of 0.0170 and a validation loss of 0.0328
- When tested on the unseen cube of data, this network had a loss of 0.621
![image](https://github.com/user-attachments/assets/5039002a-f5e5-4101-a919-cb749b639722)
![image](https://github.com/user-attachments/assets/24a12a82-b8c6-49f8-aa73-d4fb552eb521)

### Modification 18
- Keeping the original channel numbers, I used an early stopping patience of 25
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- The network stopped training at epcoh 50/1000 with a training loss of 0.0123 and a validation loss of 0.0365
- When tested on the unseen cube of data, this network had a loss of 0.556
![image](https://github.com/user-attachments/assets/3e35e8bc-a23c-4d5d-8efc-d125606164d7)
![image](https://github.com/user-attachments/assets/6f78cfc4-58fc-4666-8397-8da36864e2c5)

### Modification 19
- I doubled the original channel numbers, added batch normalization, and used an early stopping patience of 25
- The small swarm was divided into 605 cubes with 13 voxels overlap to use for training data
- The network stopped training at epcoh 90/1000 with a training loss of 0.0046 and a validation loss of 0.0303
- When tested on the unseen cube of data, this network had a loss of 0.696
![image](https://github.com/user-attachments/assets/f22164ab-2b83-4182-bf67-73722b6e3232)
![image](https://github.com/user-attachments/assets/0973ffe2-3116-45e8-9956-af8c1bdacca6)


As I mentioned last week, I believe that it's also important to look at varying threshold values to determine if a bee is present, so I reran everything with a different threshold to compare how the results looked:

Modication 9:
![image](https://github.com/user-attachments/assets/32ca4366-695a-431c-9d97-b3f8e6fe9ebd)
![image](https://github.com/user-attachments/assets/9c913bd2-8c8e-4cfc-9931-25fc10bc2ebe)

Modification 10:
![image](https://github.com/user-attachments/assets/bd9c0ab1-64a1-44a0-89c4-c9c0aacfd1da)
![image](https://github.com/user-attachments/assets/10c5024e-785b-4f16-9bf6-a6f1eb4bb8de)

Modification 11:
![image](https://github.com/user-attachments/assets/a4a6771f-ded5-4019-b0c5-5fc56348adae)
![image](https://github.com/user-attachments/assets/350cc8bc-566d-491a-93d7-c63e4cd99932)

Modification 12:


Modification 13:



Modification 14:



Modification 15:



Modifiction 16:



Modification 17:



Modification 18:



Modification 19:







