---
layout: post
title: Week 3
---


Throughout this week I continued labelling bees in the larger swarm to use for training data. I also started the week with a one-on-one meeting with Danielle to discuss my goals for this summer in more depth. Then, I spent the majority of my time making modifications to the lab's preliminary neural network. Since the starting architecture was fairly simple, there were lots of things I could do to change it and see how I could imrove it, so I spent the rest of the week making small modifications to it and seeing how the impacted the output. Below I summarize what changes I made and the key observations of each change. Note that, at this point, the neural network has only a single cube of bees to train on and no validation data, so the model is at easily at risk for overfitting. Additionally, all models were trained for 1000 epochs


Modification 0, starting structure:
- With no changes, the network had a loss of 3.182e-06 for the training data and 9.292 for test data (the smame cube of bees but cropped in a different area)
- Wall time: 30min 38s
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/a88aabe4-cc27-4832-9cf5-f4871fd0a343)



Modification 1:
- Given the starting neural network, I tried to decrease the number of channels by half at every step
- This change increased the loss from 3.182e-06 to 0.002 given the training data
- Given test data, the loss decreased from 9.292 to 3.835
- Training concluded faster (wall time: 13min 9s)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/df7141c2-9b91-4f8d-a115-dac06f0ec844)



Modification 2:
- Given the starting neural network, I inreased the number of channels by double at every step
- This change decreased the loss from 3.182e-06 to 8.884e-07 given the training data
- Given test data, the loss increased from 9.292 to 7.548
- Training took significantly longer (wall time: 1h 22min 10s)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/808d2e8e-d6ad-4705-8c80-b8974dde10c6)


At this point I determined that the higher number of channels probably improved the network's ability to learn, but was also overfitting the model to the limited training data. Since I only had one cube of labeled data to use at the moment, I didn't have more training data or validation data I could use to prevent overfitting. 

Later, I was provided with a labeled cube that one other student made and was able to use all three of the pretrained models on that cube to see how the three modifications performed when tested on completely new data.


Testing Modification 0 With Different Test Labels:
- Given new test data, loss decreased to 6.498
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/cd5b66a6-0143-4f03-9710-3443f899d0cc)


Testing Modification 1 With Different Test Labels:
- Given new test data, loss increased to 5.394
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/b8b8f2ff-65a1-4eaf-ab1c-26a2df4a687f)


Testing Modification 2 With Different Test Labels:
- Given new test data, loss decreased to 6.049
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/e015730f-5fde-467f-999c-126f94ec6db4)


Since I now had new data, I decided to retrain the models using both cubes as training data to hopefully help with the overfitting probelem I observed. After changing the model to allow for batch sizes, I trained the models using 2 batch sizes and 1000 epochs for the same three channel sizes. Even though there are now technically two cubes to use for training data, they are the same cube, just labelled differently, so I am concerned that this may just confuse the model.

Modification 0 With Two Sets of Training Labels:
- Wall time: 23min 27s
- Unlike all other times when training loss leveled out at around 500 epochs, this model's performance was still wildly fluctuating at the end of 1000 epochs: ![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/d708b8b1-9db0-4df1-9b89-93436df2238c)
- The final loss after 1000 epochs was 0.053 given the training data
- Given the same cube, cropped in a different location, the test loss was 1.312
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/63d32027-a5c0-496e-987f-c0ec6c27b29c)


Modification 1 With Two Sets of Training Labels:
- Wall time: 5min 56s
- The model was still slighly decreasing at 1000 epochs, but it was almost level and not fluctuating like the previous training ^
- The final loss after 1000 epochs was 0.057 given the training data
- Given the same cube, cropped in a different location, the test loss was 1.051
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/cc91dfc5-66e4-4e27-a535-dea656d42c25)


Modification 2 With Two Sets of Training Labels:
- Wall time: 2h 8min 43s
- The final loss after 1000 epochs was 0.053 given the training data
- Given the same cube, cropped in a different location, the test loss was 1.177, the best so far for this modification
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/22804af7-8f9d-441e-87c8-d4e2d2c42085)

Based on these three tests, it appears that training with more data, even if it is just the same starting cube but with two different sets of labels helps prevent overfitting. So I modified my code to allow for training to occur on overlapping crops of the cube. I used just my own labels, and created 27 overlapping crops of the cube to train on. I used this just to train for modification 1, since training was taking significantly longer to complete (wall time: 1h 57min). This model seemed to have a harder time learning, but after 1000 epochs reached a loss of 0.071 (which was not the lowest loss overall). Given a previously unseen crop of the same cube, this model had a test loss of 0.148, which is worse than training with a single crop but two different labels. (Note, batch size of 3 was used)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/b057f02f-6d5d-47bd-8e22-3bcacf754482)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/4614075a-4729-41bc-9a93-ffb0d048bc7a)
Since this model seemed to perform best overall at this point, I decided to also test it using the labels made by the other student. When given the same cube (still an unseen crop) but these different labels, the test loss increased to 0.290 which is still better than the other models.


At this point, I've concluded that:
1) The higher number of channels seems to improve learning, but with limited traing data easily leads to overfitting.
2) The higher number of channels also dramatically increases the amount of time it takes for learning to be completed.
3) I can synthetically create more training data by cropping my single cube in different places, which significantly imprves test loss. However, this also increases training time drastically so I won't be able to train a model like this with larger channel numbers until I can use a GPU.
4) I can also increase the amount of training data by using the same cube but two different labels for the cube. Though this doesn't seem intuitive to me, it still prevents overfitting and therefore decreases the test loss.


Some things that I think would be interesting to look into next:
- So far I've only edited the channel numbers, so adding another layer to the network, playing with the kernal size, and the step size are all things I'd like to explore
- Since I now can synthetically create more training data, setting some aside for validation data to help prevent overfitting could be interesting
- Other methods, such as dropout, to prevent overfitting could be added
- Other forms of data augmentation (ie. rotating and mirroring the cube) could help diversify training data more
- Allowing for larger sets of test data to be tested at the same time to get a better metric of how models are performing



