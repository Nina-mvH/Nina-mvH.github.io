---
layout: post
title: Week 3
---


Throughout this week I continued labelling bees in the larger swarm to use for training data. I also started the week with a one-on-one meeting with Danielle to discuss my goals for this summer in more depth. Then, I spent the majority of my time making modifications to the lab's preliminary neural network. Since the starting architecture was fairly simple, there were lots of things I could do to change it and see how I could imrove it, so I spent the rest of the week making small modifications to it and seeing how the impacted the output. Below I summarize what changes I made and the key observations of each change. Note that, at this point, the neural network has only a single cube of bees to train on, so the model is at easiyl at risk for overfitting.


Modification 0, starting structure:
- With no changes, the network has a loss of 3.182e-06 for the training data and 9.292 for test data (the smame cube of bees but cropped in a different area)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/a88aabe4-cc27-4832-9cf5-f4871fd0a343)



Modification 1:
- Given the starting neural network, I tried to decrease the number of channels by half at every step
- This change increased the loss from 3.182e-06 to 0.002 given the training data
- Given new test data, the loss decreased from 9.292 to 3.835
- Training concluded faster
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/df7141c2-9b91-4f8d-a115-dac06f0ec844)



Modification 2:
- Given the starting neural network, I inreased the number of channels by double at every step
- This change decreased the loss from 3.182e-06 to 8.884e-07 given the training data
- Given new test data, the loss increased from 9.292 to 7.548
- Training took significantly longer
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/808d2e8e-d6ad-4705-8c80-b8974dde10c6)


