---
layout: post
title: Week 3
---


Throughout this week I continued labelling bees in the larger swarm to use for training data. I also started the week with a one-on-one meeting with Danielle to discuss my goals for this summer in more depth. Then, I spent the majority of my time making modifications to the lab's preliminary neural network. Since the starting architecture was fairly simple, there were lots of things I could do to change it and see how I could imrove it, so I spent the rest of the week making small modifications to it and seeing how the impacted the output. Below I summarize what changes I made and the key observations of each change. Note that, at this point, the neural network has only a single cube of bees to train on and no validation data, so the model is at easily at risk for overfitting.


Modification 0, starting structure:
- With no changes, the network had a loss of 3.182e-06 for the training data and 9.292 for test data (the smame cube of bees but cropped in a different area)
- Wall time: 30min 38s
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/a88aabe4-cc27-4832-9cf5-f4871fd0a343)



Modification 1:
- Given the starting neural network, I tried to decrease the number of channels by half at every step
- This change increased the loss from 3.182e-06 to 0.002 given the training data
- Given test data, the loss decreased from 9.292 to 3.835
- Training concluded faster (wall time: 13min 9s)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/df7141c2-9b91-4f8d-a115-dac06f0ec844)



Modification 2:
- Given the starting neural network, I inreased the number of channels by double at every step
- This change decreased the loss from 3.182e-06 to 8.884e-07 given the training data
- Given test data, the loss increased from 9.292 to 7.548
- Training took significantly longer (wall time: 1h 22min 10s)
![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/808d2e8e-d6ad-4705-8c80-b8974dde10c6)


At this point I determined that the higher number of channels probably improved the network's ability to learn, but was also overfitting the model to the limited training data. Since I only had one cube of labeled data to use at the moment, I didn't have more training data or validation data I could use to prevent overfitting. 

Later, I was provided with a labeled cube that one other student made and was able to use all three of the pretrained models on that cube to see how the three modifications performed when tested on completely new data.


Testing Modification 0 With Different Test Labels:
- Given new test data, loss decreased to 6.498
-![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/cd5b66a6-0143-4f03-9710-3443f899d0cc)


Testing Modification 1 With Different Test Labels:
- Given new test data, loss increased to 5.394
- ![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/b8b8f2ff-65a1-4eaf-ab1c-26a2df4a687f)


Testing Modification 2 With Different Test Labels:
- Given new test data, loss decreased to 6.049
- ![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/e015730f-5fde-467f-999c-126f94ec6db4)


Since I now had new data, I decided to retrain the models using both cubes as training data to hopefully help with the overfitting probelem I observed. After changing the model to allow for batch sizes, I trained the models using 2 batch sizes and 1000 epochs for the same three channel sizes. Even though there are now technically two cubes to use for training data, they are the same cube so I am concerned that this may just confuse the model

Modification 0 With Two Training Cubes:
- Wall time: 23min 27s
- Unlike all other times when training loss leveled out at around 500 epochs, this model's performance was still wildly fluctuating at the end of 1000 epochs: ![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/d708b8b1-9db0-4df1-9b89-93436df2238c)
- The final loss after 1000 epochs was 0.053
- Given the same cube, cropped in a different location, the test loss was 1.312
- ![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/63d32027-a5c0-496e-987f-c0ec6c27b29c)


Modification 1 With Two Training Cubes:
- Wall time: 5min 56s
- The model was still slighly decreasing at 1000 epochs, but it was almost level and not fluctuating like the previous training ^
- Given the same cube, cropped in a different location, the test loss was 1.051
-![image](https://github.com/Nina-mvH/Nina-mvH.github.io/assets/133538278/cc91dfc5-66e4-4e27-a535-dea656d42c25)


Modification 2 With Two Training Cubes:

